{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 8: Simple Linear Regression\nThis script demonstrates Simple Linear Regression with one feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sample_data():\n    \"\"\"Generate sample data for linear regression\"\"\"\n    np.random.seed(42)\n    X = np.random.rand(100, 1) * 10\n    y = 2.5 * X + 5 + np.random.randn(100, 1) * 2\n    return X, y.ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_linear_regression():\n    \"\"\"Demonstrate basic linear regression\"\"\"\n    print(\"=\" * 50)\n    print(\"Basic Simple Linear Regression\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    print(f\"\\nDataset shape: {X.shape}\")\n    print(f\"X range: [{X.min():.2f}, {X.max():.2f}]\")\n    print(f\"y range: [{y.min():.2f}, {y.max():.2f}]\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Create and train model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Get parameters\n    print(f\"\\nModel Parameters:\")\n    print(f\"Coefficient (slope): {model.coef_[0]:.4f}\")\n    print(f\"Intercept: {model.intercept_:.4f}\")\n    print(f\"Equation: y = {model.coef_[0]:.4f}x + {model.intercept_:.4f}\")\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Evaluate\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"\\nModel Evaluation:\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"R\u00b2 Score: {r2:.4f}\")\n    \n    # Visualize\n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Training data and regression line\n    plt.subplot(1, 2, 1)\n    plt.scatter(X_train, y_train, alpha=0.6, label='Training Data')\n    plt.plot(X_train, model.predict(X_train), 'r-', linewidth=2, label='Regression Line')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Linear Regression - Training Data')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Test data predictions\n    plt.subplot(1, 2, 2)\n    plt.scatter(X_test, y_test, alpha=0.6, label='Actual')\n    plt.scatter(X_test, y_pred, alpha=0.6, label='Predicted')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Linear Regression - Test Predictions')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('lab8_linear_regression.png')\n    plt.close()\n    print(\"\\nLinear regression plot saved as 'lab8_linear_regression.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def residual_analysis():\n    \"\"\"Analyze residuals\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Residual Analysis\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    # Train model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Calculate residuals\n    y_pred = model.predict(X)\n    residuals = y - y_pred\n    \n    print(f\"\\nResidual Statistics:\")\n    print(f\"Mean: {residuals.mean():.6f}\")\n    print(f\"Std Dev: {residuals.std():.4f}\")\n    print(f\"Min: {residuals.min():.4f}\")\n    print(f\"Max: {residuals.max():.4f}\")\n    \n    # Visualize residuals\n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: Residual plot\n    plt.subplot(1, 2, 1)\n    plt.scatter(y_pred, residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.title('Residual Plot')\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Residual histogram\n    plt.subplot(1, 2, 2)\n    plt.hist(residuals, bins=20, edgecolor='black')\n    plt.xlabel('Residuals')\n    plt.ylabel('Frequency')\n    plt.title('Residual Distribution')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('lab8_residuals.png')\n    plt.close()\n    print(\"\\nResidual plots saved as 'lab8_residuals.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manual_implementation():\n    \"\"\"Implement linear regression manually\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Manual Linear Regression Implementation\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    # Calculate parameters manually using least squares\n    X_mean = X.mean()\n    y_mean = y.mean()\n    \n    # Calculate slope (coefficient)\n    numerator = np.sum((X.ravel() - X_mean) * (y - y_mean))\n    denominator = np.sum((X.ravel() - X_mean) ** 2)\n    slope = numerator / denominator\n    \n    # Calculate intercept\n    intercept = y_mean - slope * X_mean\n    \n    print(f\"\\nManual Calculation:\")\n    print(f\"Slope: {slope:.4f}\")\n    print(f\"Intercept: {intercept:.4f}\")\n    \n    # Compare with sklearn\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    print(f\"\\nSklearn Calculation:\")\n    print(f\"Slope: {model.coef_[0]:.4f}\")\n    print(f\"Intercept: {model.intercept_:.4f}\")\n    \n    print(f\"\\nDifference:\")\n    print(f\"Slope difference: {abs(slope - model.coef_[0]):.10f}\")\n    print(f\"Intercept difference: {abs(intercept - model.intercept_):.10f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prediction_intervals():\n    \"\"\"Calculate and visualize prediction intervals\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Prediction Intervals\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    # Train model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Make predictions\n    y_pred = model.predict(X)\n    \n    # Calculate standard error\n    residuals = y - y_pred\n    mse = np.mean(residuals ** 2)\n    std_error = np.sqrt(mse)\n    \n    print(f\"\\nStandard Error: {std_error:.4f}\")\n    \n    # Calculate prediction intervals (95% confidence)\n    confidence = 1.96  # 95% confidence interval\n    upper_bound = y_pred + confidence * std_error\n    lower_bound = y_pred - confidence * std_error\n    \n    # Visualize\n    X_sorted_idx = X.ravel().argsort()\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X, y, alpha=0.6, label='Data')\n    plt.plot(X[X_sorted_idx], y_pred[X_sorted_idx], 'r-', linewidth=2, label='Prediction')\n    plt.fill_between(X.ravel()[X_sorted_idx], \n                     lower_bound[X_sorted_idx], \n                     upper_bound[X_sorted_idx],\n                     alpha=0.2, label='95% Prediction Interval')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Linear Regression with Prediction Intervals')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab8_prediction_intervals.png')\n    plt.close()\n    print(\"\\nPrediction intervals plot saved as 'lab8_prediction_intervals.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def real_world_example():\n    \"\"\"Real-world example: predicting house prices\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Real-World Example: House Price Prediction\")\n    print(\"=\" * 50)\n    \n    # Create sample data: area (sq ft) vs price ($1000s)\n    np.random.seed(42)\n    area = np.random.uniform(500, 3000, 100).reshape(-1, 1)\n    price = 50 + 0.1 * area + np.random.normal(0, 20, (100, 1))\n    price = price.ravel()\n    \n    print(f\"\\nPredicting house price from area\")\n    print(f\"Sample size: {len(area)}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        area, price, test_size=0.2, random_state=42\n    )\n    \n    # Train model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    print(f\"\\nModel: Price = {model.coef_[0]:.4f} \u00d7 Area + {model.intercept_:.4f}\")\n    \n    # Predictions\n    y_pred = model.predict(X_test)\n    \n    # Evaluate\n    r2 = r2_score(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    \n    print(f\"\\nR\u00b2 Score: {r2:.4f}\")\n    print(f\"RMSE: ${rmse:.2f}k\")\n    \n    # Example predictions\n    sample_areas = np.array([[1000], [1500], [2000]])\n    sample_prices = model.predict(sample_areas)\n    \n    print(f\"\\nSample Predictions:\")\n    for area_val, price_val in zip(sample_areas, sample_prices):\n        print(f\"Area: {area_val[0]:.0f} sq ft \u2192 Predicted Price: ${price_val:.2f}k\")\n    \n    # Visualize\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_train, y_train, alpha=0.6, label='Training Data')\n    plt.scatter(X_test, y_test, alpha=0.6, label='Test Data')\n    \n    # Plot regression line\n    X_range = np.linspace(area.min(), area.max(), 100).reshape(-1, 1)\n    y_range = model.predict(X_range)\n    plt.plot(X_range, y_range, 'r-', linewidth=2, label='Regression Line')\n    \n    plt.xlabel('Area (sq ft)')\n    plt.ylabel('Price ($1000s)')\n    plt.title('House Price Prediction')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab8_real_world_example.png')\n    plt.close()\n    print(\"\\nReal-world example plot saved as 'lab8_real_world_example.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate simple linear regression\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 8: Simple Linear Regression\")\n    print(\"=\" * 50)\n    \n    # Basic linear regression\n    basic_linear_regression()\n    \n    # Residual analysis\n    residual_analysis()\n    \n    # Manual implementation\n    manual_implementation()\n    \n    # Prediction intervals\n    prediction_intervals()\n    \n    # Real-world example\n    real_world_example()\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 8 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}