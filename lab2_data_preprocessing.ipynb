{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 2: Data Preprocessing and Cleaning\nThis script demonstrates various data preprocessing and cleaning techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sample_data():\n    \"\"\"Create sample dataset with missing values and outliers\"\"\"\n    np.random.seed(42)\n    data = {\n        'Age': [25, 30, np.nan, 35, 28, 32, np.nan, 40, 22, 150],  # Contains missing values and outlier\n        'Salary': [50000, 60000, 55000, np.nan, 58000, 62000, 54000, 70000, np.nan, 65000],\n        'Category': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'C'],\n        'Score': [85, 90, 78, 92, 88, 75, 95, 82, 89, 91]\n    }\n    return pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_missing_values(df):\n    \"\"\"Demonstrate different techniques to handle missing values\"\"\"\n    print(\"=\" * 50)\n    print(\"Handling Missing Values\")\n    print(\"=\" * 50)\n    \n    print(\"\\nOriginal Data with Missing Values:\")\n    print(df)\n    print(f\"\\nMissing values per column:\\n{df.isnull().sum()}\")\n    \n    # Method 1: Fill with mean\n    df_mean = df.copy()\n    df_mean['Age'].fillna(df_mean['Age'].mean(), inplace=True)\n    df_mean['Salary'].fillna(df_mean['Salary'].mean(), inplace=True)\n    print(\"\\nAfter filling with mean:\")\n    print(df_mean)\n    \n    # Method 2: Fill with median\n    df_median = df.copy()\n    df_median['Age'].fillna(df_median['Age'].median(), inplace=True)\n    df_median['Salary'].fillna(df_median['Salary'].median(), inplace=True)\n    print(\"\\nAfter filling with median:\")\n    print(df_median)\n    \n    # Method 3: Using SimpleImputer\n    df_imputer = df.copy()\n    imputer = SimpleImputer(strategy='mean')\n    df_imputer[['Age', 'Salary']] = imputer.fit_transform(df_imputer[['Age', 'Salary']])\n    print(\"\\nAfter using SimpleImputer:\")\n    print(df_imputer)\n    \n    return df_imputer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_outliers(df):\n    \"\"\"Detect and handle outliers using IQR method\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Handling Outliers\")\n    print(\"=\" * 50)\n    \n    # Detect outliers using IQR\n    Q1 = df['Age'].quantile(0.25)\n    Q3 = df['Age'].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    print(f\"\\nAge statistics:\")\n    print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n    print(f\"Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n    \n    # Identify outliers\n    outliers = df[(df['Age'] < lower_bound) | (df['Age'] > upper_bound)]\n    print(f\"\\nOutliers detected:\\n{outliers}\")\n    \n    # Remove outliers\n    df_clean = df[(df['Age'] >= lower_bound) & (df['Age'] <= upper_bound)]\n    print(f\"\\nData after removing outliers:\")\n    print(df_clean)\n    \n    return df_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_categorical_variables(df):\n    \"\"\"Encode categorical variables\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Encoding Categorical Variables\")\n    print(\"=\" * 50)\n    \n    print(f\"\\nOriginal Category column:\\n{df['Category'].value_counts()}\")\n    \n    # Label Encoding\n    df_encoded = df.copy()\n    label_encoder = LabelEncoder()\n    df_encoded['Category_Encoded'] = label_encoder.fit_transform(df_encoded['Category'])\n    print(f\"\\nAfter Label Encoding:\")\n    print(df_encoded[['Category', 'Category_Encoded']])\n    \n    # One-Hot Encoding\n    df_onehot = pd.get_dummies(df, columns=['Category'], prefix='Category')\n    print(f\"\\nAfter One-Hot Encoding:\")\n    print(df_onehot)\n    \n    return df_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_scale_data(df):\n    \"\"\"Demonstrate normalization and scaling techniques\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Normalization and Scaling\")\n    print(\"=\" * 50)\n    \n    # Select numeric columns\n    numeric_cols = ['Age', 'Salary', 'Score']\n    \n    print(f\"\\nOriginal data statistics:\")\n    print(df[numeric_cols].describe())\n    \n    # Standardization (Z-score normalization)\n    scaler = StandardScaler()\n    df_standardized = df.copy()\n    df_standardized[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n    print(f\"\\nAfter Standardization (mean=0, std=1):\")\n    print(df_standardized[numeric_cols].describe())\n    \n    # Min-Max Scaling\n    minmax_scaler = MinMaxScaler()\n    df_normalized = df.copy()\n    df_normalized[numeric_cols] = minmax_scaler.fit_transform(df[numeric_cols])\n    print(f\"\\nAfter Min-Max Scaling (range 0-1):\")\n    print(df_normalized[numeric_cols].describe())\n    \n    return df_standardized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_data(df):\n    \"\"\"Split data into training and testing sets\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Data Splitting\")\n    print(\"=\" * 50)\n    \n    # Prepare features and target\n    X = df[['Age', 'Salary', 'Score']]\n    y = df['Category_Encoded'] if 'Category_Encoded' in df.columns else LabelEncoder().fit_transform(df['Category'])\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    print(f\"\\nTotal samples: {len(X)}\")\n    print(f\"Training samples: {len(X_train)}\")\n    print(f\"Testing samples: {len(X_test)}\")\n    print(f\"Train/Test split ratio: 80/20\")\n    \n    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate data preprocessing\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 2: Data Preprocessing and Cleaning\")\n    print(\"=\" * 50)\n    \n    # Create sample data\n    df = create_sample_data()\n    \n    # Handle missing values\n    df_clean = handle_missing_values(df)\n    \n    # Handle outliers\n    df_clean = handle_outliers(df_clean)\n    \n    # Encode categorical variables\n    df_encoded = encode_categorical_variables(df_clean)\n    \n    # Normalize and scale\n    df_scaled = normalize_scale_data(df_encoded)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = split_data(df_encoded)\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 2 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}