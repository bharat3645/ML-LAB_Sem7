{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 11: K-Means Clustering\nThis script demonstrates K-Means clustering algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs, load_iris\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_kmeans_clustering():\n    \"\"\"Demonstrate basic K-Means clustering\"\"\"\n    print(\"=\" * 50)\n    print(\"Basic K-Means Clustering\")\n    print(\"=\" * 50)\n    \n    # Generate synthetic data\n    X, y_true = make_blobs(n_samples=300, centers=4, n_features=2,\n                           cluster_std=0.60, random_state=42)\n    \n    print(f\"\\nDataset shape: {X.shape}\")\n    print(f\"True number of clusters: 4\")\n    \n    # Apply K-Means\n    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n    y_pred = kmeans.fit_predict(X)\n    \n    # Get cluster centers\n    centers = kmeans.cluster_centers_\n    \n    print(f\"\\nCluster Centers:\")\n    for i, center in enumerate(centers):\n        print(f\"Cluster {i}: {center}\")\n    \n    print(f\"\\nInertia (within-cluster sum of squares): {kmeans.inertia_:.2f}\")\n    print(f\"Number of iterations: {kmeans.n_iter_}\")\n    \n    # Visualize results\n    plt.figure(figsize=(12, 5))\n    \n    # Plot 1: True labels\n    plt.subplot(1, 2, 1)\n    plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6)\n    plt.title('True Labels')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.colorbar(label='Cluster')\n    \n    # Plot 2: K-Means clusters\n    plt.subplot(1, 2, 2)\n    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)\n    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8,\n                marker='X', edgecolors='black', linewidth=2, label='Centroids')\n    plt.title('K-Means Clusters')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.colorbar(label='Cluster')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('lab11_basic_kmeans.png')\n    plt.close()\n    print(\"\\nBasic K-Means plot saved as 'lab11_basic_kmeans.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def elbow_method():\n    \"\"\"Use elbow method to find optimal number of clusters\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Elbow Method for Optimal K\")\n    print(\"=\" * 50)\n    \n    # Generate synthetic data\n    X, _ = make_blobs(n_samples=300, centers=4, n_features=2,\n                      cluster_std=0.60, random_state=42)\n    \n    # Test different values of K\n    K_range = range(1, 11)\n    inertias = []\n    \n    print(\"\\nTesting different K values:\")\n    for k in K_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        kmeans.fit(X)\n        inertias.append(kmeans.inertia_)\n        print(f\"K={k}: Inertia={kmeans.inertia_:.2f}\")\n    \n    # Plot elbow curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n    plt.xlabel('Number of Clusters (K)')\n    plt.ylabel('Inertia (Within-cluster sum of squares)')\n    plt.title('Elbow Method for Optimal K')\n    plt.grid(True, alpha=0.3)\n    plt.xticks(K_range)\n    \n    # Mark the elbow point (K=4)\n    plt.axvline(x=4, color='r', linestyle='--', label='Optimal K=4')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('lab11_elbow_method.png')\n    plt.close()\n    print(\"\\nElbow method plot saved as 'lab11_elbow_method.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def silhouette_analysis():\n    \"\"\"Use silhouette score to evaluate clustering quality\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Silhouette Analysis\")\n    print(\"=\" * 50)\n    \n    # Generate synthetic data\n    X, _ = make_blobs(n_samples=300, centers=4, n_features=2,\n                      cluster_std=0.60, random_state=42)\n    \n    # Test different values of K\n    K_range = range(2, 11)\n    silhouette_scores = []\n    \n    print(\"\\nSilhouette Scores for different K:\")\n    for k in K_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X)\n        score = silhouette_score(X, labels)\n        silhouette_scores.append(score)\n        print(f\"K={k}: Silhouette Score={score:.4f}\")\n    \n    # Find optimal K\n    optimal_k = K_range[np.argmax(silhouette_scores)]\n    print(f\"\\nOptimal K (highest silhouette score): {optimal_k}\")\n    \n    # Plot silhouette scores\n    plt.figure(figsize=(10, 6))\n    plt.plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n    plt.axvline(x=optimal_k, color='r', linestyle='--', \n                label=f'Optimal K={optimal_k}')\n    plt.xlabel('Number of Clusters (K)')\n    plt.ylabel('Silhouette Score')\n    plt.title('Silhouette Analysis')\n    plt.grid(True, alpha=0.3)\n    plt.xticks(K_range)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('lab11_silhouette_analysis.png')\n    plt.close()\n    print(\"\\nSilhouette analysis plot saved as 'lab11_silhouette_analysis.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iris_clustering():\n    \"\"\"Apply K-Means to Iris dataset\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"K-Means on Iris Dataset\")\n    print(\"=\" * 50)\n    \n    # Load Iris dataset\n    iris = load_iris()\n    X = iris.data\n    y_true = iris.target\n    \n    print(f\"\\nDataset shape: {X.shape}\")\n    print(f\"True number of species: {len(np.unique(y_true))}\")\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Apply K-Means\n    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n    y_pred = kmeans.fit_predict(X_scaled)\n    \n    # Evaluate\n    silhouette = silhouette_score(X_scaled, y_pred)\n    davies_bouldin = davies_bouldin_score(X_scaled, y_pred)\n    \n    print(f\"\\nSilhouette Score: {silhouette:.4f}\")\n    print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n    print(\"(Lower Davies-Bouldin is better)\")\n    \n    # Visualize using first 2 features\n    plt.figure(figsize=(15, 5))\n    \n    # Plot 1: True labels\n    plt.subplot(1, 3, 1)\n    plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6)\n    plt.xlabel(iris.feature_names[0])\n    plt.ylabel(iris.feature_names[1])\n    plt.title('True Species Labels')\n    plt.colorbar(label='Species')\n    \n    # Plot 2: K-Means clusters\n    plt.subplot(1, 3, 2)\n    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)\n    centers_original = scaler.inverse_transform(kmeans.cluster_centers_)\n    plt.scatter(centers_original[:, 0], centers_original[:, 1], \n                c='red', s=200, alpha=0.8, marker='X', \n                edgecolors='black', linewidth=2, label='Centroids')\n    plt.xlabel(iris.feature_names[0])\n    plt.ylabel(iris.feature_names[1])\n    plt.title('K-Means Clusters')\n    plt.colorbar(label='Cluster')\n    plt.legend()\n    \n    # Plot 3: Different features\n    plt.subplot(1, 3, 3)\n    plt.scatter(X[:, 2], X[:, 3], c=y_pred, cmap='viridis', s=50, alpha=0.6)\n    plt.scatter(centers_original[:, 2], centers_original[:, 3], \n                c='red', s=200, alpha=0.8, marker='X', \n                edgecolors='black', linewidth=2, label='Centroids')\n    plt.xlabel(iris.feature_names[2])\n    plt.ylabel(iris.feature_names[3])\n    plt.title('K-Means (Different Features)')\n    plt.colorbar(label='Cluster')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('lab11_iris_clustering.png')\n    plt.close()\n    print(\"\\nIris clustering plot saved as 'lab11_iris_clustering.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kmeans_convergence():\n    \"\"\"Demonstrate K-Means convergence process\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"K-Means Convergence Process\")\n    print(\"=\" * 50)\n    \n    # Generate simple data\n    np.random.seed(42)\n    X, _ = make_blobs(n_samples=100, centers=3, n_features=2,\n                      cluster_std=0.60, random_state=42)\n    \n    # Initialize K-Means with max_iter=1 to see step-by-step\n    print(\"\\nShowing convergence iterations:\")\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n    \n    for i, max_iter in enumerate([0, 1, 2, 3, 5, 10]):\n        kmeans = KMeans(n_clusters=3, random_state=42, max_iter=max_iter, n_init=1)\n        \n        if max_iter == 0:\n            # Show initial random centroids\n            kmeans.fit(X)\n            labels = np.zeros(len(X), dtype=int)\n            centers = kmeans.cluster_centers_\n        else:\n            labels = kmeans.fit_predict(X)\n            centers = kmeans.cluster_centers_\n        \n        axes[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)\n        axes[i].scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8,\n                       marker='X', edgecolors='black', linewidth=2)\n        axes[i].set_title(f'Iteration {max_iter}')\n        axes[i].set_xlabel('Feature 1')\n        axes[i].set_ylabel('Feature 2')\n        \n        print(f\"Iteration {max_iter}: Inertia = {kmeans.inertia_:.2f}\")\n    \n    plt.tight_layout()\n    plt.savefig('lab11_convergence.png')\n    plt.close()\n    print(\"\\nConvergence plot saved as 'lab11_convergence.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_initializations():\n    \"\"\"Compare different initialization methods\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Comparing Initialization Methods\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, _ = make_blobs(n_samples=300, centers=4, n_features=2,\n                      cluster_std=0.60, random_state=42)\n    \n    # Test different initializations\n    init_methods = ['k-means++', 'random']\n    results = {}\n    \n    for init in init_methods:\n        kmeans = KMeans(n_clusters=4, init=init, n_init=10, random_state=42)\n        kmeans.fit(X)\n        \n        results[init] = {\n            'inertia': kmeans.inertia_,\n            'n_iter': kmeans.n_iter_\n        }\n        \n        print(f\"\\n{init}:\")\n        print(f\"  Inertia: {kmeans.inertia_:.2f}\")\n        print(f\"  Iterations: {kmeans.n_iter_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cluster_analysis():\n    \"\"\"Analyze characteristics of each cluster\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Cluster Characteristics Analysis\")\n    print(\"=\" * 50)\n    \n    # Load Iris dataset\n    iris = load_iris()\n    X = iris.data\n    \n    # Apply K-Means\n    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n    clusters = kmeans.fit_predict(X)\n    \n    # Create DataFrame for analysis\n    df = pd.DataFrame(X, columns=iris.feature_names)\n    df['Cluster'] = clusters\n    \n    print(\"\\nCluster Statistics:\")\n    print(df.groupby('Cluster').mean())\n    \n    print(\"\\nCluster Sizes:\")\n    print(df['Cluster'].value_counts().sort_index())\n    \n    # Visualize cluster characteristics\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.ravel()\n    \n    for i, feature in enumerate(iris.feature_names):\n        for cluster in range(3):\n            cluster_data = df[df['Cluster'] == cluster][feature]\n            axes[i].hist(cluster_data, alpha=0.5, label=f'Cluster {cluster}', bins=15)\n        \n        axes[i].set_xlabel(feature)\n        axes[i].set_ylabel('Frequency')\n        axes[i].set_title(f'{feature} Distribution by Cluster')\n        axes[i].legend()\n        axes[i].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('lab11_cluster_characteristics.png')\n    plt.close()\n    print(\"\\nCluster characteristics plot saved as 'lab11_cluster_characteristics.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate K-Means clustering\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 11: K-Means Clustering\")\n    print(\"=\" * 50)\n    \n    # Basic K-Means\n    basic_kmeans_clustering()\n    \n    # Elbow method\n    elbow_method()\n    \n    # Silhouette analysis\n    silhouette_analysis()\n    \n    # Iris clustering\n    iris_clustering()\n    \n    # Convergence process\n    kmeans_convergence()\n    \n    # Compare initializations\n    compare_initializations()\n    \n    # Cluster analysis\n    cluster_analysis()\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 11 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}