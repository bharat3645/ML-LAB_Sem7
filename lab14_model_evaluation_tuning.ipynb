{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 14: Model Evaluation and Tuning\nThis script demonstrates model evaluation metrics and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer, load_iris\nfrom sklearn.model_selection import (train_test_split, cross_val_score, \n                                     GridSearchCV, RandomizedSearchCV, learning_curve)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                            f1_score, roc_auc_score, roc_curve, \n                            classification_report, confusion_matrix)\nimport seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_classification_metrics():\n    \"\"\"Demonstrate various classification metrics\"\"\"\n    print(\"=\" * 50)\n    print(\"Classification Metrics\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    cancer = load_breast_cancer()\n    X = cancer.data\n    y = cancer.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train_scaled, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test_scaled)\n    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred_proba)\n    \n    print(\"\\nEvaluation Metrics:\")\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1-Score:  {f1:.4f}\")\n    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n    \n    print(\"\\nDetailed Classification Report:\")\n    print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=cancer.target_names,\n                yticklabels=cancer.target_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig('lab14_confusion_matrix.png')\n    plt.close()\n    print(\"\\nConfusion matrix saved as 'lab14_confusion_matrix.png'\")\n    \n    return y_test, y_pred_proba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_roc_curves(y_test, y_pred_proba):\n    \"\"\"Plot ROC curves\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"ROC Curve Analysis\")\n    print(\"=\" * 50)\n    \n    # Calculate ROC curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    roc_auc = roc_auc_score(y_test, y_pred_proba)\n    \n    print(f\"\\nArea Under ROC Curve: {roc_auc:.4f}\")\n    \n    # Plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, \n             label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab14_roc_curve.png')\n    plt.close()\n    print(\"\\nROC curve saved as 'lab14_roc_curve.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validation_evaluation():\n    \"\"\"Demonstrate cross-validation\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Cross-Validation Evaluation\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Create model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    # Perform k-fold cross-validation\n    cv_scores = cross_val_score(model, X, y, cv=5)\n    \n    print(\"\\n5-Fold Cross-Validation Results:\")\n    print(f\"Scores: {cv_scores}\")\n    print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n    print(f\"Standard Deviation: {cv_scores.std():.4f}\")\n    print(f\"95% Confidence Interval: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, \"\n          f\"{cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n    \n    # Visualize\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='skyblue', edgecolor='black')\n    plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', \n                label=f'Mean: {cv_scores.mean():.4f}')\n    plt.xlabel('Fold')\n    plt.ylabel('Accuracy')\n    plt.title('Cross-Validation Scores')\n    plt.legend()\n    plt.grid(True, alpha=0.3, axis='y')\n    plt.tight_layout()\n    plt.savefig('lab14_cross_validation.png')\n    plt.close()\n    print(\"\\nCross-validation plot saved as 'lab14_cross_validation.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grid_search_tuning():\n    \"\"\"Demonstrate Grid Search for hyperparameter tuning\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Grid Search Hyperparameter Tuning\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Define parameter grid\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [3, 5, 7, None],\n        'min_samples_split': [2, 5, 10]\n    }\n    \n    print(\"\\nParameter Grid:\")\n    for param, values in param_grid.items():\n        print(f\"  {param}: {values}\")\n    \n    # Create model\n    rf = RandomForestClassifier(random_state=42)\n    \n    # Perform Grid Search\n    print(\"\\nPerforming Grid Search...\")\n    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    \n    print(\"\\nBest Parameters:\")\n    for param, value in grid_search.best_params_.items():\n        print(f\"  {param}: {value}\")\n    \n    print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.4f}\")\n    \n    # Evaluate on test set\n    best_model = grid_search.best_estimator_\n    test_score = best_model.score(X_test, y_test)\n    print(f\"Test Set Score: {test_score:.4f}\")\n    \n    # Visualize results\n    results = pd.DataFrame(grid_search.cv_results_)\n    \n    # Plot top configurations\n    results_sorted = results.sort_values('mean_test_score', ascending=False).head(10)\n    \n    plt.figure(figsize=(12, 6))\n    x_pos = range(len(results_sorted))\n    plt.barh(x_pos, results_sorted['mean_test_score'], color='lightblue', edgecolor='black')\n    plt.yticks(x_pos, [f\"Config {i+1}\" for i in range(len(results_sorted))])\n    plt.xlabel('Mean Cross-Validation Score')\n    plt.title('Top 10 Hyperparameter Configurations')\n    plt.grid(True, alpha=0.3, axis='x')\n    plt.tight_layout()\n    plt.savefig('lab14_grid_search.png')\n    plt.close()\n    print(\"\\nGrid search results saved as 'lab14_grid_search.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_search_tuning():\n    \"\"\"Demonstrate Random Search for hyperparameter tuning\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Random Search Hyperparameter Tuning\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    cancer = load_breast_cancer()\n    X = cancer.data\n    y = cancer.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Define parameter distribution\n    param_dist = {\n        'C': [0.1, 1, 10, 100],\n        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n        'kernel': ['rbf', 'poly']\n    }\n    \n    print(\"\\nParameter Distribution:\")\n    for param, values in param_dist.items():\n        print(f\"  {param}: {values}\")\n    \n    # Create model\n    svc = SVC(random_state=42)\n    \n    # Perform Random Search\n    print(\"\\nPerforming Random Search (20 iterations)...\")\n    random_search = RandomizedSearchCV(\n        svc, param_dist, n_iter=20, cv=3, \n        scoring='accuracy', random_state=42, n_jobs=-1\n    )\n    random_search.fit(X_train, y_train)\n    \n    print(\"\\nBest Parameters:\")\n    for param, value in random_search.best_params_.items():\n        print(f\"  {param}: {value}\")\n    \n    print(f\"\\nBest Cross-Validation Score: {random_search.best_score_:.4f}\")\n    \n    # Evaluate on test set\n    best_model = random_search.best_estimator_\n    test_score = best_model.score(X_test, y_test)\n    print(f\"Test Set Score: {test_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def learning_curve_analysis():\n    \"\"\"Analyze learning curves to detect overfitting/underfitting\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Learning Curve Analysis\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    cancer = load_breast_cancer()\n    X = cancer.data\n    y = cancer.target\n    \n    # Create model\n    model = DecisionTreeClassifier(random_state=42)\n    \n    # Calculate learning curve\n    train_sizes, train_scores, val_scores = learning_curve(\n        model, X, y, cv=5, n_jobs=-1,\n        train_sizes=np.linspace(0.1, 1.0, 10),\n        scoring='accuracy'\n    )\n    \n    # Calculate mean and std\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n    \n    print(\"\\nLearning Curve Summary:\")\n    print(f\"Final Training Score: {train_mean[-1]:.4f} (+/- {train_std[-1]:.4f})\")\n    print(f\"Final Validation Score: {val_mean[-1]:.4f} (+/- {val_std[-1]:.4f})\")\n    \n    # Plot learning curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n                     alpha=0.1, color='blue')\n    plt.plot(train_sizes, val_mean, 'o-', color='green', label='Validation Score')\n    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n                     alpha=0.1, color='green')\n    plt.xlabel('Training Set Size')\n    plt.ylabel('Accuracy')\n    plt.title('Learning Curve')\n    plt.legend(loc='best')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab14_learning_curve.png')\n    plt.close()\n    print(\"\\nLearning curve saved as 'lab14_learning_curve.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_models():\n    \"\"\"Compare different models\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Model Comparison\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Define models\n    models = {\n        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n        'Decision Tree': DecisionTreeClassifier(random_state=42),\n        'SVM': SVC(random_state=42)\n    }\n    \n    # Evaluate each model\n    results = {}\n    print(\"\\nEvaluating models with 5-fold cross-validation:\")\n    \n    for name, model in models.items():\n        scores = cross_val_score(model, X, y, cv=5)\n        results[name] = {\n            'mean': scores.mean(),\n            'std': scores.std(),\n            'scores': scores\n        }\n        print(f\"\\n{name}:\")\n        print(f\"  Mean Accuracy: {scores.mean():.4f}\")\n        print(f\"  Std Dev: {scores.std():.4f}\")\n    \n    # Visualize comparison\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Bar plot\n    model_names = list(results.keys())\n    means = [results[name]['mean'] for name in model_names]\n    stds = [results[name]['std'] for name in model_names]\n    \n    ax1.bar(model_names, means, yerr=stds, capsize=5, \n            color=['blue', 'green', 'orange'], alpha=0.7, edgecolor='black')\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title('Model Comparison (Mean \u00b1 Std)')\n    ax1.grid(True, alpha=0.3, axis='y')\n    ax1.set_ylim([0.9, 1.0])\n    \n    # Box plot\n    all_scores = [results[name]['scores'] for name in model_names]\n    ax2.boxplot(all_scores, labels=model_names)\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Model Comparison (Distribution)')\n    ax2.grid(True, alpha=0.3, axis='y')\n    ax2.set_ylim([0.9, 1.0])\n    \n    plt.tight_layout()\n    plt.savefig('lab14_model_comparison.png')\n    plt.close()\n    print(\"\\nModel comparison plot saved as 'lab14_model_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bias_variance_tradeoff():\n    \"\"\"Demonstrate bias-variance tradeoff\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Bias-Variance Tradeoff\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    cancer = load_breast_cancer()\n    X = cancer.data\n    y = cancer.target\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Test different max_depth values\n    max_depths = range(1, 21)\n    train_scores = []\n    test_scores = []\n    \n    print(\"\\nTesting different model complexities:\")\n    for depth in max_depths:\n        model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n        model.fit(X_train, y_train)\n        \n        train_score = model.score(X_train, y_train)\n        test_score = model.score(X_test, y_test)\n        \n        train_scores.append(train_score)\n        test_scores.append(test_score)\n        \n        if depth % 5 == 0:\n            print(f\"Max Depth {depth}: Train={train_score:.4f}, Test={test_score:.4f}\")\n    \n    # Plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(max_depths, train_scores, 'o-', label='Training Score')\n    plt.plot(max_depths, test_scores, 's-', label='Test Score')\n    plt.xlabel('Model Complexity (Max Depth)')\n    plt.ylabel('Accuracy')\n    plt.title('Bias-Variance Tradeoff')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    # Mark regions\n    plt.axvspan(1, 5, alpha=0.1, color='red', label='High Bias (Underfitting)')\n    plt.axvspan(15, 20, alpha=0.1, color='blue', label='High Variance (Overfitting)')\n    \n    plt.tight_layout()\n    plt.savefig('lab14_bias_variance.png')\n    plt.close()\n    print(\"\\nBias-variance plot saved as 'lab14_bias_variance.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate model evaluation and tuning\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 14: Model Evaluation and Tuning\")\n    print(\"=\" * 50)\n    \n    # Evaluation metrics\n    y_test, y_pred_proba = evaluate_classification_metrics()\n    \n    # ROC curves\n    plot_roc_curves(y_test, y_pred_proba)\n    \n    # Cross-validation\n    cross_validation_evaluation()\n    \n    # Grid search\n    grid_search_tuning()\n    \n    # Random search\n    random_search_tuning()\n    \n    # Learning curves\n    learning_curve_analysis()\n    \n    # Model comparison\n    compare_models()\n    \n    # Bias-variance tradeoff\n    bias_variance_tradeoff()\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 14 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}