{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 4: K-Nearest Neighbors (KNN) Classifier\nThis script demonstrates KNN classification algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris, make_classification\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_knn_classification():\n    \"\"\"Demonstrate basic KNN classification\"\"\"\n    print(\"=\" * 50)\n    print(\"Basic KNN Classification on Iris Dataset\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    print(f\"\\nDataset shape: {X.shape}\")\n    print(f\"Classes: {iris.target_names}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Create and train KNN classifier\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X_train_scaled, y_train)\n    \n    # Make predictions\n    y_pred = knn.predict(X_test_scaled)\n    \n    # Evaluate\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"\\nAccuracy: {accuracy:.4f}\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=iris.target_names,\n                yticklabels=iris.target_names)\n    plt.title('Confusion Matrix - KNN Classifier')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig('lab4_confusion_matrix.png')\n    plt.close()\n    print(\"\\nConfusion matrix saved as 'lab4_confusion_matrix.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_optimal_k():\n    \"\"\"Find optimal value of K\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Finding Optimal K Value\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Split and scale data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Test different K values\n    k_values = range(1, 31)\n    train_scores = []\n    test_scores = []\n    \n    for k in k_values:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        knn.fit(X_train_scaled, y_train)\n        \n        train_score = knn.score(X_train_scaled, y_train)\n        test_score = knn.score(X_test_scaled, y_test)\n        \n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    # Find optimal K\n    optimal_k = k_values[np.argmax(test_scores)]\n    print(f\"\\nOptimal K value: {optimal_k}\")\n    print(f\"Best test accuracy: {max(test_scores):.4f}\")\n    \n    # Plot results\n    plt.figure(figsize=(10, 6))\n    plt.plot(k_values, train_scores, 'o-', label='Training Accuracy')\n    plt.plot(k_values, test_scores, 's-', label='Test Accuracy')\n    plt.axvline(x=optimal_k, color='r', linestyle='--', \n                label=f'Optimal K = {optimal_k}')\n    plt.xlabel('K Value')\n    plt.ylabel('Accuracy')\n    plt.title('KNN: Accuracy vs K Value')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab4_optimal_k.png')\n    plt.close()\n    print(\"\\nOptimal K plot saved as 'lab4_optimal_k.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_distance_metrics():\n    \"\"\"Compare different distance metrics\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Comparing Distance Metrics\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Split and scale data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Test different metrics\n    metrics = ['euclidean', 'manhattan', 'minkowski']\n    results = {}\n    \n    for metric in metrics:\n        knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n        knn.fit(X_train_scaled, y_train)\n        accuracy = knn.score(X_test_scaled, y_test)\n        results[metric] = accuracy\n        print(f\"\\n{metric.capitalize()} distance: {accuracy:.4f}\")\n    \n    # Visualize comparison\n    plt.figure(figsize=(10, 6))\n    plt.bar(results.keys(), results.values(), color=['blue', 'green', 'orange'])\n    plt.ylabel('Accuracy')\n    plt.title('KNN: Comparison of Distance Metrics')\n    plt.ylim([0.9, 1.0])\n    for i, (metric, acc) in enumerate(results.items()):\n        plt.text(i, acc + 0.005, f'{acc:.4f}', ha='center')\n    plt.tight_layout()\n    plt.savefig('lab4_distance_metrics.png')\n    plt.close()\n    print(\"\\nDistance metrics comparison saved as 'lab4_distance_metrics.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn_with_cross_validation():\n    \"\"\"Demonstrate KNN with cross-validation\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"KNN with Cross-Validation\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Test with different K values using cross-validation\n    k_values = [3, 5, 7, 9, 11]\n    \n    print(\"\\nCross-validation results (5-fold):\")\n    for k in k_values:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        scores = cross_val_score(knn, X_scaled, y, cv=5)\n        print(f\"K={k}: Mean accuracy = {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decision_boundary_visualization():\n    \"\"\"Visualize decision boundaries\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Decision Boundary Visualization\")\n    print(\"=\" * 50)\n    \n    # Load iris and use only 2 features for visualization\n    iris = load_iris()\n    X = iris.data[:, :2]  # Use only first two features\n    y = iris.target\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Train KNN\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X_scaled, y)\n    \n    # Create mesh for decision boundary\n    h = 0.02\n    x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n    y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Predict on mesh\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Plot\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='RdYlBu', \n                edgecolors='black', s=50)\n    plt.xlabel(iris.feature_names[0])\n    plt.ylabel(iris.feature_names[1])\n    plt.title('KNN Decision Boundaries (K=5)')\n    plt.colorbar(label='Class')\n    plt.tight_layout()\n    plt.savefig('lab4_decision_boundary.png')\n    plt.close()\n    print(\"\\nDecision boundary saved as 'lab4_decision_boundary.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate KNN classifier\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 4: K-Nearest Neighbors (KNN) Classifier\")\n    print(\"=\" * 50)\n    \n    # Basic KNN classification\n    basic_knn_classification()\n    \n    # Find optimal K\n    find_optimal_k()\n    \n    # Compare distance metrics\n    compare_distance_metrics()\n    \n    # Cross-validation\n    knn_with_cross_validation()\n    \n    # Visualize decision boundaries\n    decision_boundary_visualization()\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 4 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}