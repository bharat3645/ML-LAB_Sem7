{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 13: Implement a Basic Artificial Neural Network\nThis script demonstrates implementation of a basic ANN using TensorFlow/Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits, load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n\n# Try to import tensorflow/keras, provide fallback message if not available\ntry:\n    from tensorflow import keras\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Dropout\n    from tensorflow.keras.utils import to_categorical\n    KERAS_AVAILABLE = True\nexcept ImportError:\n    KERAS_AVAILABLE = False\n    print(\"TensorFlow/Keras not available. Install with: pip install tensorflow\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_neural_network():\n    \"\"\"Demonstrate a simple neural network for classification\"\"\"\n    if not KERAS_AVAILABLE:\n        print(\"\\nSkipping: TensorFlow/Keras required\")\n        return\n    \n    print(\"=\" * 50)\n    print(\"Simple Neural Network\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    print(f\"\\nDataset: Iris\")\n    print(f\"Input shape: {X.shape}\")\n    print(f\"Number of classes: {len(np.unique(y))}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Convert labels to categorical\n    y_train_cat = to_categorical(y_train, num_classes=3)\n    y_test_cat = to_categorical(y_test, num_classes=3)\n    \n    # Create neural network\n    model = Sequential([\n        Dense(8, activation='relu', input_shape=(4,)),\n        Dense(6, activation='relu'),\n        Dense(3, activation='softmax')\n    ])\n    \n    # Compile model\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(\"\\nModel Architecture:\")\n    model.summary()\n    \n    # Train model\n    print(\"\\nTraining model...\")\n    history = model.fit(\n        X_train_scaled, y_train_cat,\n        epochs=100,\n        batch_size=16,\n        validation_split=0.2,\n        verbose=0\n    )\n    \n    # Evaluate\n    test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n    \n    # Make predictions\n    y_pred_proba = model.predict(X_test_scaled, verbose=0)\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n    \n    # Plot training history\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model Accuracy')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('lab13_training_history.png')\n    plt.close()\n    print(\"\\nTraining history saved as 'lab13_training_history.png'\")\n    \n    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def deep_neural_network():\n    \"\"\"Demonstrate a deeper neural network\"\"\"\n    if not KERAS_AVAILABLE:\n        print(\"\\nSkipping: TensorFlow/Keras required\")\n        return\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Deep Neural Network with Dropout\")\n    print(\"=\" * 50)\n    \n    # Load digits dataset\n    digits = load_digits()\n    X = digits.data\n    y = digits.target\n    \n    print(f\"\\nDataset: Handwritten Digits\")\n    print(f\"Input shape: {X.shape}\")\n    print(f\"Number of classes: {len(np.unique(y))}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Normalize features\n    X_train_norm = X_train / 16.0\n    X_test_norm = X_test / 16.0\n    \n    # Convert labels to categorical\n    y_train_cat = to_categorical(y_train, num_classes=10)\n    y_test_cat = to_categorical(y_test, num_classes=10)\n    \n    # Create deep neural network with dropout\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(64,)),\n        Dropout(0.3),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(32, activation='relu'),\n        Dense(10, activation='softmax')\n    ])\n    \n    # Compile model\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(\"\\nModel Architecture:\")\n    model.summary()\n    \n    # Train model\n    print(\"\\nTraining model...\")\n    history = model.fit(\n        X_train_norm, y_train_cat,\n        epochs=50,\n        batch_size=32,\n        validation_split=0.2,\n        verbose=0\n    )\n    \n    # Evaluate\n    test_loss, test_accuracy = model.evaluate(X_test_norm, y_test_cat, verbose=0)\n    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n    \n    # Make predictions\n    y_pred_proba = model.predict(X_test_norm, verbose=0)\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix - Deep Neural Network')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig('lab13_deep_nn_confusion.png')\n    plt.close()\n    print(\"\\nConfusion matrix saved as 'lab13_deep_nn_confusion.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def activation_functions_comparison():\n    \"\"\"Compare different activation functions\"\"\"\n    if not KERAS_AVAILABLE:\n        print(\"\\nSkipping: TensorFlow/Keras required\")\n        return\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Comparing Activation Functions\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Prepare data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    y_train_cat = to_categorical(y_train, num_classes=3)\n    y_test_cat = to_categorical(y_test, num_classes=3)\n    \n    # Test different activation functions\n    activations = ['relu', 'tanh', 'sigmoid']\n    results = {}\n    \n    for activation in activations:\n        model = Sequential([\n            Dense(8, activation=activation, input_shape=(4,)),\n            Dense(6, activation=activation),\n            Dense(3, activation='softmax')\n        ])\n        \n        model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        history = model.fit(\n            X_train_scaled, y_train_cat,\n            epochs=100,\n            batch_size=16,\n            validation_split=0.2,\n            verbose=0\n        )\n        \n        _, accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n        results[activation] = accuracy\n        \n        print(f\"\\n{activation.upper()}: Test Accuracy = {accuracy:.4f}\")\n    \n    # Visualize comparison\n    plt.figure(figsize=(10, 6))\n    plt.bar(results.keys(), results.values(), color=['blue', 'green', 'orange'])\n    plt.ylabel('Test Accuracy')\n    plt.title('Comparison of Activation Functions')\n    plt.ylim([0.9, 1.0])\n    for i, (act, acc) in enumerate(results.items()):\n        plt.text(i, acc + 0.005, f'{acc:.4f}', ha='center')\n    plt.tight_layout()\n    plt.savefig('lab13_activation_comparison.png')\n    plt.close()\n    print(\"\\nActivation comparison saved as 'lab13_activation_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_weights():\n    \"\"\"Visualize neural network weights\"\"\"\n    if not KERAS_AVAILABLE:\n        print(\"\\nSkipping: TensorFlow/Keras required\")\n        return\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Visualizing Network Weights\")\n    print(\"=\" * 50)\n    \n    # Create and train a simple model\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    y_cat = to_categorical(y, num_classes=3)\n    \n    model = Sequential([\n        Dense(4, activation='relu', input_shape=(4,)),\n        Dense(3, activation='softmax')\n    ])\n    \n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X_scaled, y_cat, epochs=50, verbose=0)\n    \n    # Get weights\n    weights = model.get_weights()\n    \n    print(f\"\\nLayer 1 weights shape: {weights[0].shape}\")\n    print(f\"Layer 1 bias shape: {weights[1].shape}\")\n    print(f\"Layer 2 weights shape: {weights[2].shape}\")\n    print(f\"Layer 2 bias shape: {weights[3].shape}\")\n    \n    # Visualize first layer weights\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(weights[0], cmap='coolwarm', center=0, \n                xticklabels=[f'Neuron {i+1}' for i in range(4)],\n                yticklabels=iris.feature_names)\n    plt.title('First Layer Weights')\n    plt.xlabel('Hidden Layer Neurons')\n    plt.ylabel('Input Features')\n    plt.tight_layout()\n    plt.savefig('lab13_weights_visualization.png')\n    plt.close()\n    print(\"\\nWeights visualization saved as 'lab13_weights_visualization.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manual_neural_network():\n    \"\"\"Implement a simple neural network manually\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Manual Neural Network Implementation\")\n    print(\"=\" * 50)\n    \n    # Activation functions\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n    \n    def sigmoid_derivative(x):\n        return x * (1 - x)\n    \n    # Generate simple XOR dataset\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([[0], [1], [1], [0]])\n    \n    print(\"\\nTraining on XOR problem:\")\n    print(\"Input:\\n\", X)\n    print(\"Expected Output:\\n\", y)\n    \n    # Initialize weights\n    np.random.seed(42)\n    input_neurons = 2\n    hidden_neurons = 4\n    output_neurons = 1\n    \n    weights_input_hidden = np.random.uniform(-1, 1, (input_neurons, hidden_neurons))\n    weights_hidden_output = np.random.uniform(-1, 1, (hidden_neurons, output_neurons))\n    \n    learning_rate = 0.5\n    epochs = 10000\n    \n    # Training\n    errors = []\n    for epoch in range(epochs):\n        # Forward propagation\n        hidden_input = np.dot(X, weights_input_hidden)\n        hidden_output = sigmoid(hidden_input)\n        \n        final_input = np.dot(hidden_output, weights_hidden_output)\n        final_output = sigmoid(final_input)\n        \n        # Calculate error\n        error = y - final_output\n        errors.append(np.mean(np.abs(error)))\n        \n        # Backpropagation\n        d_output = error * sigmoid_derivative(final_output)\n        error_hidden = d_output.dot(weights_hidden_output.T)\n        d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n        \n        # Update weights\n        weights_hidden_output += hidden_output.T.dot(d_output) * learning_rate\n        weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n    \n    print(f\"\\nTraining complete after {epochs} epochs\")\n    print(f\"Final error: {errors[-1]:.6f}\")\n    \n    print(\"\\nPredictions:\")\n    for i in range(len(X)):\n        hidden = sigmoid(np.dot(X[i], weights_input_hidden))\n        output = sigmoid(np.dot(hidden, weights_hidden_output))\n        print(f\"Input: {X[i]} \u2192 Output: {output[0]:.4f} (Expected: {y[i][0]})\")\n    \n    # Plot error over epochs\n    plt.figure(figsize=(10, 6))\n    plt.plot(errors)\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Absolute Error')\n    plt.title('Training Error Over Time (Manual NN)')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab13_manual_nn_training.png')\n    plt.close()\n    print(\"\\nManual NN training plot saved as 'lab13_manual_nn_training.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate artificial neural networks\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 13: Artificial Neural Network\")\n    print(\"=\" * 50)\n    \n    # Simple neural network\n    simple_neural_network()\n    \n    # Deep neural network\n    deep_neural_network()\n    \n    # Activation functions comparison\n    activation_functions_comparison()\n    \n    # Visualize weights\n    visualize_weights()\n    \n    # Manual implementation\n    manual_neural_network()\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 13 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}