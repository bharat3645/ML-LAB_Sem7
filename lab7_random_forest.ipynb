{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 7: Random Forest Classifier\nThis script demonstrates Random Forest classification algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris, load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_random_forest():\n    \"\"\"Demonstrate basic Random Forest classifier\"\"\"\n    print(\"=\" * 50)\n    print(\"Basic Random Forest Classification\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    print(f\"\\nDataset: Iris\")\n    print(f\"Shape: {X.shape}\")\n    print(f\"Classes: {iris.target_names}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Create and train Random Forest\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = rf.predict(X_test)\n    \n    # Evaluate\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"\\nAccuracy: {accuracy:.4f}\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=iris.target_names,\n                yticklabels=iris.target_names)\n    plt.title('Confusion Matrix - Random Forest')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig('lab7_confusion_matrix.png')\n    plt.close()\n    print(\"\\nConfusion matrix saved as 'lab7_confusion_matrix.png'\")\n    \n    # Feature importance\n    print(\"\\nFeature Importances:\")\n    for name, importance in zip(iris.feature_names, rf.feature_importances_):\n        print(f\"{name}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize_n_estimators():\n    \"\"\"Find optimal number of trees\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Optimizing Number of Trees\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    cancer = load_breast_cancer()\n    X = cancer.data\n    y = cancer.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Test different number of estimators\n    n_estimators_range = range(10, 201, 10)\n    train_scores = []\n    test_scores = []\n    \n    print(\"\\nTesting different numbers of trees:\")\n    for n_est in n_estimators_range:\n        rf = RandomForestClassifier(n_estimators=n_est, random_state=42)\n        rf.fit(X_train, y_train)\n        \n        train_score = rf.score(X_train, y_train)\n        test_score = rf.score(X_test, y_test)\n        \n        train_scores.append(train_score)\n        test_scores.append(test_score)\n        \n        if n_est % 50 == 0:\n            print(f\"Trees {n_est}: Train={train_score:.4f}, Test={test_score:.4f}\")\n    \n    # Find optimal number\n    optimal_n = n_estimators_range[np.argmax(test_scores)]\n    print(f\"\\nOptimal number of trees: {optimal_n}\")\n    print(f\"Best test accuracy: {max(test_scores):.4f}\")\n    \n    # Plot results\n    plt.figure(figsize=(10, 6))\n    plt.plot(n_estimators_range, train_scores, 'o-', label='Training Accuracy')\n    plt.plot(n_estimators_range, test_scores, 's-', label='Test Accuracy')\n    plt.axvline(x=optimal_n, color='r', linestyle='--',\n                label=f'Optimal n = {optimal_n}')\n    plt.xlabel('Number of Trees')\n    plt.ylabel('Accuracy')\n    plt.title('Random Forest: Accuracy vs Number of Trees')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab7_n_estimators.png')\n    plt.close()\n    print(\"\\nOptimal trees plot saved as 'lab7_n_estimators.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize_max_depth():\n    \"\"\"Find optimal maximum depth\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Optimizing Maximum Depth\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Test different max_depth values\n    max_depths = [None, 3, 5, 7, 10, 15, 20]\n    train_scores = []\n    test_scores = []\n    depth_labels = []\n    \n    print(\"\\nTesting different max_depth values:\")\n    for depth in max_depths:\n        rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n        rf.fit(X_train, y_train)\n        \n        train_score = rf.score(X_train, y_train)\n        test_score = rf.score(X_test, y_test)\n        \n        train_scores.append(train_score)\n        test_scores.append(test_score)\n        depth_label = 'None' if depth is None else str(depth)\n        depth_labels.append(depth_label)\n        \n        print(f\"Depth {depth_label}: Train={train_score:.4f}, Test={test_score:.4f}\")\n    \n    # Plot results\n    plt.figure(figsize=(10, 6))\n    x_pos = range(len(depth_labels))\n    plt.plot(x_pos, train_scores, 'o-', label='Training Accuracy')\n    plt.plot(x_pos, test_scores, 's-', label='Test Accuracy')\n    plt.xticks(x_pos, depth_labels)\n    plt.xlabel('Max Depth')\n    plt.ylabel('Accuracy')\n    plt.title('Random Forest: Accuracy vs Max Depth')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab7_max_depth.png')\n    plt.close()\n    print(\"\\nMax depth plot saved as 'lab7_max_depth.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def feature_importance_analysis():\n    \"\"\"Analyze feature importances\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Feature Importance Analysis\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    cancer = load_breast_cancer()\n    X = cancer.data\n    y = cancer.target\n    \n    # Train random forest\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X, y)\n    \n    # Get feature importances\n    importances = rf.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    \n    # Print top 10 features\n    print(\"\\nTop 10 Most Important Features:\")\n    for i in range(min(10, len(indices))):\n        print(f\"{i+1}. {cancer.feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n    \n    # Plot feature importances\n    plt.figure(figsize=(12, 6))\n    plt.bar(range(10), importances[indices[:10]])\n    plt.xticks(range(10), [cancer.feature_names[i] for i in indices[:10]], \n               rotation=45, ha='right')\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.title('Top 10 Feature Importances - Random Forest')\n    plt.tight_layout()\n    plt.savefig('lab7_feature_importance.png')\n    plt.close()\n    print(\"\\nFeature importance plot saved as 'lab7_feature_importance.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_with_single_tree():\n    \"\"\"Compare Random Forest with single Decision Tree\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Random Forest vs Single Decision Tree\")\n    print(\"=\" * 50)\n    \n    from sklearn.tree import DecisionTreeClassifier\n    \n    # Load dataset\n    cancer = load_breast_cancer()\n    X = cancer.data\n    y = cancer.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Train Decision Tree\n    dt = DecisionTreeClassifier(random_state=42)\n    dt.fit(X_train, y_train)\n    dt_accuracy = dt.score(X_test, y_test)\n    \n    # Train Random Forest\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X_train, y_train)\n    rf_accuracy = rf.score(X_test, y_test)\n    \n    print(f\"\\nDecision Tree Accuracy: {dt_accuracy:.4f}\")\n    print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n    print(f\"Improvement: {(rf_accuracy - dt_accuracy):.4f}\")\n    \n    # Visualize comparison\n    plt.figure(figsize=(10, 6))\n    models = ['Decision Tree', 'Random Forest']\n    accuracies = [dt_accuracy, rf_accuracy]\n    plt.bar(models, accuracies, color=['blue', 'green'])\n    plt.ylabel('Accuracy')\n    plt.title('Decision Tree vs Random Forest')\n    plt.ylim([0.9, 1.0])\n    for i, acc in enumerate(accuracies):\n        plt.text(i, acc + 0.005, f'{acc:.4f}', ha='center')\n    plt.tight_layout()\n    plt.savefig('lab7_dt_vs_rf.png')\n    plt.close()\n    print(\"\\nComparison plot saved as 'lab7_dt_vs_rf.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def out_of_bag_score():\n    \"\"\"Demonstrate Out-of-Bag (OOB) score\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Out-of-Bag (OOB) Score\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Train Random Forest with OOB score\n    rf = RandomForestClassifier(\n        n_estimators=100,\n        oob_score=True,\n        random_state=42\n    )\n    rf.fit(X, y)\n    \n    print(f\"\\nOOB Score: {rf.oob_score_:.4f}\")\n    print(\"\\nNote: OOB score is an estimate of accuracy without using a test set\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_validation():\n    \"\"\"Perform cross-validation on Random Forest\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Cross-Validation\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Create classifier\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    # Perform cross-validation\n    scores = cross_val_score(rf, X, y, cv=5)\n    \n    print(f\"\\nCross-validation scores: {scores}\")\n    print(f\"Mean accuracy: {scores.mean():.4f}\")\n    print(f\"Standard deviation: {scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_bootstrap_samples():\n    \"\"\"Analyze bootstrap sampling effect\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Bootstrap Sampling Analysis\")\n    print(\"=\" * 50)\n    \n    # Load dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n    \n    # Compare with and without bootstrap\n    print(\"\\nWith Bootstrap (default):\")\n    rf_with_bootstrap = RandomForestClassifier(\n        n_estimators=100,\n        bootstrap=True,\n        random_state=42\n    )\n    rf_with_bootstrap.fit(X_train, y_train)\n    print(f\"Accuracy: {rf_with_bootstrap.score(X_test, y_test):.4f}\")\n    \n    print(\"\\nWithout Bootstrap:\")\n    rf_without_bootstrap = RandomForestClassifier(\n        n_estimators=100,\n        bootstrap=False,\n        random_state=42\n    )\n    rf_without_bootstrap.fit(X_train, y_train)\n    print(f\"Accuracy: {rf_without_bootstrap.score(X_test, y_test):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate Random Forest classifier\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 7: Random Forest Classifier\")\n    print(\"=\" * 50)\n    \n    # Basic random forest\n    basic_random_forest()\n    \n    # Optimize n_estimators\n    optimize_n_estimators()\n    \n    # Optimize max_depth\n    optimize_max_depth()\n    \n    # Feature importance\n    feature_importance_analysis()\n    \n    # Compare with single tree\n    compare_with_single_tree()\n    \n    # OOB score\n    out_of_bag_score()\n    \n    # Cross-validation\n    cross_validation()\n    \n    # Bootstrap analysis\n    analyze_bootstrap_samples()\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 7 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}