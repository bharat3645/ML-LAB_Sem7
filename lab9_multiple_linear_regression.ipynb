{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lab 9: Multiple Linear Regression\nThis script demonstrates Multiple Linear Regression with multiple features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom mpl_toolkits.mplot3d import Axes3D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sample_data():\n    \"\"\"Generate sample data with multiple features\"\"\"\n    np.random.seed(42)\n    n_samples = 100\n    X1 = np.random.rand(n_samples) * 10\n    X2 = np.random.rand(n_samples) * 5\n    X3 = np.random.rand(n_samples) * 3\n    \n    # y = 2*X1 + 3*X2 - 1*X3 + 5 + noise\n    y = 2 * X1 + 3 * X2 - 1 * X3 + 5 + np.random.randn(n_samples) * 2\n    \n    X = np.column_stack([X1, X2, X3])\n    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_multiple_regression():\n    \"\"\"Demonstrate basic multiple linear regression\"\"\"\n    print(\"=\" * 50)\n    print(\"Basic Multiple Linear Regression\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    print(f\"\\nDataset shape: {X.shape}\")\n    print(f\"Number of features: {X.shape[1]}\")\n    print(f\"Number of samples: {X.shape[0]}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Create and train model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Get parameters\n    print(f\"\\nModel Parameters:\")\n    print(f\"Coefficients: {model.coef_}\")\n    print(f\"Intercept: {model.intercept_:.4f}\")\n    \n    equation = f\"y = {model.intercept_:.4f}\"\n    for i, coef in enumerate(model.coef_):\n        equation += f\" + {coef:.4f}*X{i+1}\"\n    print(f\"\\nEquation: {equation}\")\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Evaluate\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"\\nModel Evaluation:\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n    print(f\"R\u00b2 Score: {r2:.4f}\")\n    \n    # Visualize predictions vs actual\n    plt.figure(figsize=(10, 6))\n    plt.scatter(range(len(y_test)), y_test, alpha=0.6, label='Actual')\n    plt.scatter(range(len(y_pred)), y_pred, alpha=0.6, label='Predicted')\n    plt.xlabel('Sample Index')\n    plt.ylabel('y')\n    plt.title('Multiple Linear Regression: Actual vs Predicted')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab9_predictions.png')\n    plt.close()\n    print(\"\\nPredictions plot saved as 'lab9_predictions.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def feature_importance():\n    \"\"\"Analyze feature importance through coefficients\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Feature Importance Analysis\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    # Standardize features for fair comparison\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Train model on scaled data\n    model = LinearRegression()\n    model.fit(X_scaled, y)\n    \n    # Get coefficients\n    coefficients = model.coef_\n    feature_names = [f'Feature {i+1}' for i in range(len(coefficients))]\n    \n    print(f\"\\nStandardized Coefficients:\")\n    for name, coef in zip(feature_names, coefficients):\n        print(f\"{name}: {coef:.4f}\")\n    \n    # Visualize feature importance\n    plt.figure(figsize=(10, 6))\n    plt.bar(feature_names, np.abs(coefficients), color=['blue', 'green', 'orange'])\n    plt.xlabel('Features')\n    plt.ylabel('Absolute Coefficient Value')\n    plt.title('Feature Importance (Absolute Coefficients)')\n    for i, coef in enumerate(coefficients):\n        plt.text(i, np.abs(coef) + 0.05, f'{coef:.4f}', ha='center')\n    plt.tight_layout()\n    plt.savefig('lab9_feature_importance.png')\n    plt.close()\n    print(\"\\nFeature importance plot saved as 'lab9_feature_importance.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def three_d_visualization():\n    \"\"\"Visualize regression with 2 features in 3D\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"3D Visualization (2 Features)\")\n    print(\"=\" * 50)\n    \n    # Generate data with 2 features for visualization\n    np.random.seed(42)\n    n_samples = 100\n    X1 = np.random.rand(n_samples) * 10\n    X2 = np.random.rand(n_samples) * 10\n    y = 2 * X1 + 3 * X2 + 5 + np.random.randn(n_samples) * 2\n    \n    X = np.column_stack([X1, X2])\n    \n    # Train model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    print(f\"\\nModel: y = {model.coef_[0]:.4f}*X1 + {model.coef_[1]:.4f}*X2 + {model.intercept_:.4f}\")\n    \n    # Create mesh for surface plot\n    x1_range = np.linspace(X1.min(), X1.max(), 20)\n    x2_range = np.linspace(X2.min(), X2.max(), 20)\n    x1_mesh, x2_mesh = np.meshgrid(x1_range, x2_range)\n    \n    # Predict on mesh\n    X_mesh = np.column_stack([x1_mesh.ravel(), x2_mesh.ravel()])\n    y_mesh = model.predict(X_mesh).reshape(x1_mesh.shape)\n    \n    # 3D plot\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Plot data points\n    ax.scatter(X1, X2, y, c='blue', marker='o', alpha=0.6, label='Data')\n    \n    # Plot surface\n    ax.plot_surface(x1_mesh, x2_mesh, y_mesh, alpha=0.3, cmap='viridis')\n    \n    ax.set_xlabel('X1')\n    ax.set_ylabel('X2')\n    ax.set_zlabel('y')\n    ax.set_title('Multiple Linear Regression (3D)')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('lab9_3d_visualization.png', dpi=100)\n    plt.close()\n    print(\"\\n3D visualization saved as 'lab9_3d_visualization.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def residual_analysis():\n    \"\"\"Analyze residuals for multiple regression\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Residual Analysis\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    # Train model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Calculate residuals\n    y_pred = model.predict(X)\n    residuals = y - y_pred\n    \n    print(f\"\\nResidual Statistics:\")\n    print(f\"Mean: {residuals.mean():.6f}\")\n    print(f\"Std Dev: {residuals.std():.4f}\")\n    print(f\"Min: {residuals.min():.4f}\")\n    print(f\"Max: {residuals.max():.4f}\")\n    \n    # Visualize residuals\n    plt.figure(figsize=(15, 5))\n    \n    # Plot 1: Residuals vs Predicted\n    plt.subplot(1, 3, 1)\n    plt.scatter(y_pred, residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.title('Residuals vs Predicted')\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 2: Residuals vs Feature 1\n    plt.subplot(1, 3, 2)\n    plt.scatter(X[:, 0], residuals, alpha=0.6)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('Feature 1')\n    plt.ylabel('Residuals')\n    plt.title('Residuals vs Feature 1')\n    plt.grid(True, alpha=0.3)\n    \n    # Plot 3: Residual histogram\n    plt.subplot(1, 3, 3)\n    plt.hist(residuals, bins=20, edgecolor='black')\n    plt.xlabel('Residuals')\n    plt.ylabel('Frequency')\n    plt.title('Residual Distribution')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('lab9_residuals.png')\n    plt.close()\n    print(\"\\nResidual plots saved as 'lab9_residuals.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def real_world_example():\n    \"\"\"Real-world example: predicting car prices\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Real-World Example: Car Price Prediction\")\n    print(\"=\" * 50)\n    \n    # Create sample data: age, mileage, engine size vs price\n    np.random.seed(42)\n    n = 100\n    age = np.random.uniform(1, 15, n)  # years\n    mileage = np.random.uniform(5000, 150000, n)  # miles\n    engine_size = np.random.uniform(1.0, 4.0, n)  # liters\n    \n    # Price based on features with some noise\n    price = (30000 - 1500 * age - 0.1 * mileage + 3000 * engine_size + \n             np.random.normal(0, 2000, n))\n    \n    X = np.column_stack([age, mileage, engine_size])\n    feature_names = ['Age (years)', 'Mileage (miles)', 'Engine Size (L)']\n    \n    print(f\"\\nPredicting car price from multiple features\")\n    print(f\"Sample size: {n}\")\n    print(f\"Features: {', '.join(feature_names)}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, price, test_size=0.2, random_state=42\n    )\n    \n    # Train model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    print(f\"\\nModel Equation:\")\n    equation = f\"Price = ${model.intercept_:.2f}\"\n    for i, (name, coef) in enumerate(zip(feature_names, model.coef_)):\n        sign = '+' if coef >= 0 else ''\n        equation += f\" {sign} {coef:.2f} \u00d7 {name}\"\n    print(equation)\n    \n    # Predictions\n    y_pred = model.predict(X_test)\n    \n    # Evaluate\n    r2 = r2_score(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    \n    print(f\"\\nR\u00b2 Score: {r2:.4f}\")\n    print(f\"RMSE: ${rmse:.2f}\")\n    \n    # Example predictions\n    sample_cars = np.array([\n        [3, 30000, 2.0],   # 3 years old, 30k miles, 2L engine\n        [8, 80000, 1.5],   # 8 years old, 80k miles, 1.5L engine\n        [1, 10000, 3.5]    # 1 year old, 10k miles, 3.5L engine\n    ])\n    sample_prices = model.predict(sample_cars)\n    \n    print(f\"\\nSample Predictions:\")\n    for car, pred_price in zip(sample_cars, sample_prices):\n        print(f\"Age: {car[0]:.0f}y, Mileage: {car[1]:.0f}mi, Engine: {car[2]:.1f}L \"\n              f\"\u2192 Predicted Price: ${pred_price:.2f}\")\n    \n    # Visualize\n    plt.figure(figsize=(10, 6))\n    plt.scatter(y_test, y_pred, alpha=0.6)\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n             'r--', linewidth=2, label='Perfect Prediction')\n    plt.xlabel('Actual Price ($)')\n    plt.ylabel('Predicted Price ($)')\n    plt.title('Car Price Prediction: Actual vs Predicted')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('lab9_real_world_example.png')\n    plt.close()\n    print(\"\\nReal-world example plot saved as 'lab9_real_world_example.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_models():\n    \"\"\"Compare models with different number of features\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Comparing Models with Different Features\")\n    print(\"=\" * 50)\n    \n    # Generate data\n    X, y = generate_sample_data()\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Test models with increasing number of features\n    results = {}\n    \n    for n_features in range(1, X.shape[1] + 1):\n        X_train_subset = X_train[:, :n_features]\n        X_test_subset = X_test[:, :n_features]\n        \n        model = LinearRegression()\n        model.fit(X_train_subset, y_train)\n        \n        train_score = model.score(X_train_subset, y_train)\n        test_score = model.score(X_test_subset, y_test)\n        \n        results[n_features] = {'train': train_score, 'test': test_score}\n        \n        print(f\"\\nWith {n_features} feature(s):\")\n        print(f\"  Training R\u00b2: {train_score:.4f}\")\n        print(f\"  Test R\u00b2: {test_score:.4f}\")\n    \n    # Visualize\n    n_features_list = list(results.keys())\n    train_scores = [results[n]['train'] for n in n_features_list]\n    test_scores = [results[n]['test'] for n in n_features_list]\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(n_features_list, train_scores, 'o-', label='Training R\u00b2')\n    plt.plot(n_features_list, test_scores, 's-', label='Test R\u00b2')\n    plt.xlabel('Number of Features')\n    plt.ylabel('R\u00b2 Score')\n    plt.title('Model Performance vs Number of Features')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.xticks(n_features_list)\n    plt.tight_layout()\n    plt.savefig('lab9_feature_comparison.png')\n    plt.close()\n    print(\"\\nFeature comparison plot saved as 'lab9_feature_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n    \"\"\"Main function to demonstrate multiple linear regression\"\"\"\n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 9: Multiple Linear Regression\")\n    print(\"=\" * 50)\n    \n    # Basic multiple regression\n    basic_multiple_regression()\n    \n    # Feature importance\n    feature_importance()\n    \n    # 3D visualization\n    three_d_visualization()\n    \n    # Residual analysis\n    residual_analysis()\n    \n    # Real-world example\n    real_world_example()\n    \n    # Compare models\n    compare_models()\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(\"Lab 9 Complete!\")\n    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}